{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Recognition/classification CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pre-trained CNN and its related preprocessing method\n",
    "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, GlobalAveragePooling2D, MaxPooling2D, SpatialDropout2D\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process dataset\n",
    "\n",
    "The dataset is formatted into image array list and labels list, and are subsequntly split into train, test and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def load_data(fpath):    \n",
    "    img=Image.open(fpath).resize((71,71)) \n",
    "    img = np.asarray(img, dtype='float32')\n",
    "\n",
    "    return img\n",
    "\n",
    "# Load images as np arrays for easier manipulation\n",
    "rootdir = 'C:/Users/patri/Desktop/characters/chars/'\n",
    "print(rootdir)\n",
    "images=[]\n",
    "labels=[]\n",
    "total=34000\n",
    "j=1\n",
    "i=0\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        images.append(load_data(os.path.join(subdir, file)))\n",
    "        labels.append(j)\n",
    "        i += 1\n",
    "        if i % 1000 == 0: j += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "images = np.asarray(images) \n",
    "labels = np.asarray(labels).reshape(34000,1) \n",
    "images.shape\n",
    "\n",
    "# Split data into training and test. Since this is a very small dataset, a 85/15 split was deemed best to avoid overfitting.\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=104)\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing images into train and test folders by creating images from arrays\n",
    "import cv2\n",
    "import numpy as np\n",
    "def create_images(data, labels, folder):\n",
    "    dirname=folder\n",
    "    \n",
    "    if not os.path.exists(dirname):\n",
    "      os.mkdir(dirname)\n",
    "    n=0\n",
    "    \n",
    "    for i in data:\n",
    "      label_n=labels[n]\n",
    "      subfolder = folder + \"/\" + str(label_n) \n",
    "      if not os.path.exists(subfolder):\n",
    "          os.mkdir(subfolder)  \n",
    "      filepath =  subfolder + \"/\" + str(n)+ \".jpg\"\n",
    "      cv2.imwrite(filepath, data[n]) \n",
    "      n+=1\n",
    "\n",
    "# Save images to corresponding subfolders\n",
    "# create_images(X_train, y_train, 'C:/Users/patri/Desktop/characters/train/') \n",
    "# create_images(X_test, y_test, 'C:/Users/patri/Desktop/characters/test/')\n",
    "create_images(X_val, y_val, 'C:/Users/patri/Desktop/characters/validation/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise datagenerators for image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir =os.path.realpath('C:/Users/patri/Desktop/characters/train/')\n",
    "test_dir = os.path.realpath('C:/Users/patri/Desktop/characters/test/')\n",
    "\n",
    "image_size = 71\n",
    "\n",
    "# Configure data augmentation parameters\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      horizontal_flip=True,\n",
    "      rotation_range=10,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      fill_mode='nearest'\n",
    "      )\n",
    " \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_batchsize = 32\n",
    "val_batchsize = 10\n",
    " \n",
    "# Fit the train and test images onto the data generators to augment them.\n",
    "train_generator = train_datagen.flow_from_directory( \n",
    "        train_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=train_batchsize,\n",
    "       )\n",
    " \n",
    "test_generator = validation_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CNN model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(71, 71, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Add fully connected layers and final output layer\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(34, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers,optimizers\n",
    "import tensorflow.keras.applications.vgg16\n",
    "from tensorflow.python.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from tensorflow.python.keras.layers import Input, Dense\n",
    "\n",
    "checkpoint_path = 'C:/Users/patri/Desktop/characters/model_checkpoints/model.{epoch:02d}-acc{val_accuracy:.4f}.h5'\n",
    "\n",
    "# LR schedule - reduce learning rate on loss plateau\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=4, min_lr=0.0001)\n",
    "\n",
    "callbacks = [reduce_lr, tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5), tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1)]\n",
    "model = create_model()\n",
    "\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      shuffle=True,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=50,\n",
    "      validation_data=test_generator,\n",
    "      validation_steps=test_generator.samples/test_generator.batch_size,\n",
    "      callbacks=callbacks,\n",
    "      verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-58d91383cafb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m  \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mplot_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# print('training acc.:', history.history['accuracy'][-1], '\\n','test acc.:', (history.history['val_accuracy'])[-1])\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('C:/Users/patri/Desktop/characters/model_checkpoints/model.10-acc1.0000.h5')\n",
    "\n",
    "# plot accuracy hisory\n",
    "def plot_history(history):\n",
    " plt.figure()\n",
    " plt.xlabel('Epoch')\n",
    " plt.ylabel('Accuracy %')\n",
    " plt.plot(history.epoch, np.array(history.history['accuracy']),\n",
    " label='Train Accuracy')\n",
    " plt.plot(history.epoch, np.array(history.history['val_accuracy']),\n",
    " label = 'Val Accuracy')\n",
    " plt.legend()\n",
    " plt.ylim([0.5, 1])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numberplate: ['9', 'R', 'A', 'L', 'K', 'X']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from sklearn import metrics\n",
    "%matplotlib inline \n",
    "\n",
    "dictionary = {0:'0', 1:'1', 2 :'2', 3:'3', 4:'4', 5:'5', 6:'6', 7:'7', 8:'8', 9:'9', 10:'A',\n",
    "11:'B', 12:'C', 13:'D', 14:'E', 15:'F', 16:'G', 17:'H', 18:'I', 19:'J', 20:'K',\n",
    "21:'L', 22:'M', 23:'N', 24:'P', 25:'Q', 26:'R', 27:'S', 28:'T', 29:'U',\n",
    "30:'V', 31:'W', 32:'X', 33:'Y', 34:'Z'}\n",
    "\n",
    "def cnnCharRecognition(img):\n",
    "    image = img / 255.0\n",
    "    image = np.reshape(image, (1,71,71,3))\n",
    "    new_predictions = loaded_model.predict(image)\n",
    "    char = np.argmax(new_predictions)\n",
    "    return dictionary[char]\n",
    "\n",
    "path = \"C:/Users/patri/Desktop/characters/val_plate/\"\n",
    "numberplate = []\n",
    "counter = 0\n",
    "for image_path in os.listdir(path):\n",
    "    full_image_path = os.path.join(path, image_path)\n",
    "    img=Image.open(full_image_path).resize((71,71)) \n",
    "    img = np.asarray(img, dtype='float32')\n",
    "    pred = cnnCharRecognition(img)\n",
    "    numberplate.append(pred)\n",
    "\n",
    "print(\"Numberplate: \" + str(numberplate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character localization and image pre-processing for real numberplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 3, 3, 6), (0, 5, 7, 15), (1, 21, 6, 12), (0, 0, 14, 30), (31, 3, 12, 24), (0, 13, 11, 11), (15, 33, 12, 13), (48, 11, 14, 32), (36, 10, 27, 33), (78, 9, 29, 33), (111, 9, 28, 33), (143, 9, 27, 33), (175, 9, 26, 34), (205, 7, 28, 37), (4, 4, 62, 57), (9, 13, 34, 40), (49, 14, 34, 44), (105, 15, 36, 42), (145, 18, 36, 40), (186, 20, 34, 40), (228, 15, 23, 46), (239, 14, 24, 50), (283, 18, 24, 46)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "# Apply canny edge detection \n",
    "def auto_canny(image, sigma=0.33):\n",
    "    v = np.median(image)\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    edged_image = cv2.Canny(image, lower, upper)\n",
    " \n",
    "    return edged_image\n",
    "\n",
    "# Crops characters out of numerplate \n",
    "def crop_ctrs(img):\n",
    "    ret, mask = cv2.threshold(grayimage, 254, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    cv2.imshow('mask', mask)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    image, contours, hierarchy = cv2.findContours(mask, cv2.RETR_LIST, \n",
    "    cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for contour in contours:\n",
    "\n",
    "        if cv2.contourArea(contour) < 200:\n",
    "            continue\n",
    "\n",
    "        rect = cv2.minAreaRect(contour)\n",
    "        box = cv2.boxPoints(rect)\n",
    "\n",
    "        ext_left = tuple(contour[contour[:, :, 0].argmin()][0])\n",
    "        ext_right = tuple(contour[contour[:, :, 0].argmax()][0])\n",
    "        ext_top = tuple(contour[contour[:, :, 1].argmin()][0])\n",
    "        ext_bot = tuple(contour[contour[:, :, 1].argmax()][0])\n",
    "\n",
    "        roi_corners = np.array([box], dtype=np.int32)\n",
    "\n",
    "        cv2.polylines(bounding_box_image, roi_corners, 1, (255, 0, 0), 3)\n",
    "        cv2.imshow('image', bounding_box_image)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "        cropped_image = grayimage[ext_top[1]:ext_bot[1], ext_left[0]:ext_right[0]]\n",
    "        cv2.imwrite('crop.jpg', cropped_image)\n",
    "\n",
    "path = \"C:/Users/patri/Desktop/characters/test_plate/\"\n",
    "bounding_boxes = []\n",
    "counter = 0\n",
    "for image_path in os.listdir(path):\n",
    "    if counter < 200:\n",
    "        full_image_path = os.path.join(path, image_path)\n",
    "        img = cv2.imread(full_image_path)\n",
    "\n",
    "        # Loop through each image, apply pre-processing & localize onto each character\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        thresh_inv = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY_INV,39,1)\n",
    "        edges = auto_canny(thresh_inv)\n",
    "        ctrs, _ = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        sorted_ctrs = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "\n",
    "        img_area = img.shape[0]*img.shape[1]\n",
    "\n",
    "        # Get bounding box co-ordinates for image cropping\n",
    "        for i, ctr in enumerate(sorted_ctrs):\n",
    "            x, y, w, h = cv2.boundingRect(ctr)\n",
    "            roi_area = w*h\n",
    "            roi_ratio = roi_area/img_area\n",
    "\n",
    "            if((roi_ratio >= 0.04) and (roi_ratio < 0.16)):\n",
    "                    if ((h>0.9*w) and (2.8*w>=h)):\n",
    "                        cv2.rectangle(img,(x,y),( x + w, y + h ),(90,0,255), 1)\n",
    "                        bounding_boxes.append((x,y,w,h))\n",
    "                        counter += 1\n",
    "\n",
    "print(bounding_boxes)\n",
    "# Crop bounding boxes and save into new dir\n",
    "count = 0\n",
    "for box in bounding_boxes:\n",
    "        x,y,w,h = box\n",
    "        ROI = img[y:y+h, x:x+w]\n",
    "        cv2.imwrite(\"C:/Users/patri/Desktop/characters/test_plate/char_{}.png\".format(str(count)), ROI)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitead7e005c7884182918ed2a7d9766a1f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
